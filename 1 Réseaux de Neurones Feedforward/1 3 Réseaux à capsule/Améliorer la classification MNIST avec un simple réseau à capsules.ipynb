{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f068d962-84a4-4476-8ae0-8a0b58083f38",
   "metadata": {},
   "source": [
    "# Améliorer la classification MNIST avec un simple réseau à capsules."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ebb7b95-5c17-4d86-9c03-b8a9e9a5908f",
   "metadata": {},
   "source": [
    "###############\n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "###############\n",
    "###############\n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "###############\n",
    "###############\n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "###############\n",
    "###############\n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "##############################\n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "EN COURS \n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26698c-b120-475a-a8c1-16736540ea2a",
   "metadata": {},
   "source": [
    "- dans ce notebook nous importons des vétements labelisés et nous allons entrainer un modele de convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c440280-c299-4dc9-9320-d5ecc30439c0",
   "metadata": {},
   "source": [
    "# Sommaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c362c0-212c-4bba-9eec-41099daa86f9",
   "metadata": {},
   "source": [
    "\n",
    "#### [Importation](#mon-titre1)\n",
    "#### [Load Data and normalisation](#mon-titre2)\n",
    "- [Load the data drom MNSIT FASHION](#mon-sous-titre21)\n",
    "- [Normalisation for the Neural network](#mon-sous-titre22)\n",
    "- [Print Shape](#mon-sous-titre23)\n",
    "#### [Redimension](#mon-titre3)\n",
    "#### [Model Creation](#mon-titre4)\n",
    "- [Model Representation](#mon-sous-titre41)\n",
    "- [Model Layers Type](#mon-sous-titre42)\n",
    "- [Model python](#mon-sous-titre43)\n",
    "#### [Evaluation of the model](#mon-titre5)\n",
    "#### [Test on value](#mon-titre6)\n",
    "- [Classe name list](#mon-sous-titre61)\n",
    "- [Select random image and label](#mon-sous-titre62)\n",
    "- [Prediction](#mon-sous-titre63)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b21a01-c81e-4dad-b5dc-bb492268443b",
   "metadata": {},
   "source": [
    "<a id=\"mon-titre1\"></a>\n",
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee14c08-919f-4d8b-9eca-a63bf4e14cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091454ca-d593-44c4-b54e-cdc54d4049a8",
   "metadata": {},
   "source": [
    "<a id=\"mon-titre2\"></a>\n",
    "## Load Data and normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7add3-2475-4f83-a73b-348f9fa93e2c",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre21\"></a>\n",
    "### Load the data drom MNSIT FASHION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58794254-444a-47ac-854c-90dd3cd2e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926c78c-8bd7-4b0e-a8ab-d1bcfce2b24b",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre22\"></a>\n",
    "### Normalisation for the Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a7e577-d33e-4b12-bc6b-1778ba83899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
    "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f210b7-cbb8-444b-87f8-97f54bb01aad",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre23\"></a>\n",
    "### Print Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dab5c5bf-ed71-4a5c-ba3f-a9a268b2f8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3262cb-e300-468f-b787-63d63372f792",
   "metadata": {},
   "source": [
    "<a id=\"mon-titre3\"></a>\n",
    "## Redimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9864fc1d-1e08-4369-a067-bb2adf604956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redimensionner les images pour ajouter un canal (important pour les CNN) ((nombre d'images, hauteur, largeur, nombre de canaux))\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94918d-3ccb-42eb-8a3f-99f1c5013e5a",
   "metadata": {},
   "source": [
    "<a id=\"mon-titre4\"></a>\n",
    "## Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a803945-18a9-44d9-9209-b6bd5a1f424b",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre41\"></a>\n",
    "### Model representation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de9d8d-bdb9-4fde-9cff-614e65e954e3",
   "metadata": {},
   "source": [
    "<img src=\"../../images/1_2_model.webp\" alt=\"Description alternative de l'image\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a50e9-5ddc-4349-8cb8-2995d38e4ea5",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre42\"></a>\n",
    "### Model Layers Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b4c1a9-a6a5-47a9-90bc-612b96442f60",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td style=\"width: 50%\">\n",
    "        <img src=\"../../images/Layer_conv2D.png\" alt=\"Layer_conv2D\" style=\"width: 100%\"/>\n",
    "    </td>\n",
    "    <td style=\"width: 50%; vertical-align: top;\">\n",
    "        <h2><strong>Conv2D</strong></h2>\n",
    "        <p><strong>Fonction :</strong> Couche de convolution.</p>\n",
    "        <p><strong>Paramètres :</strong> 32 filtres, taille du noyau 3x3.</p>\n",
    "        <p><strong>Activation :</strong> 'relu' (Rectified Linear Unit).</p>\n",
    "        <p><strong>Input shape :</strong> (28, 28, 1) - images de 28x28 avec 1 canal (niveaux de gris).</p>\n",
    "        <p><strong>Rôle :</strong> Détecte des caractéristiques spatiales dans l'image en appliquant 32 filtres différents.</p>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td style=\"width: 50%\">\n",
    "        <img src=\"../../images/Layer_maxPooling2.png\" alt=\"Layer_maxPooling2\" style=\"width: 100%\"/>\n",
    "    </td>\n",
    "    <td style=\"width: 50%; vertical-align: top;\">\n",
    "        <h2><strong>MaxPooling2D</strong></h2>\n",
    "        <p><strong>Taille du pool :</strong> 2x2.</p>\n",
    "        <p><strong>Rôle :</strong> Réduit la dimensionnalité spatiale (hauteur, largeur) de l'entrée, conservant les caractéristiques importantes.</p>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td style=\"width: 50%\">\n",
    "        <img src=\"../../images/Layer_flatten.png\" alt=\"Layer_flatten\" style=\"width: 100%\"/>\n",
    "    </td>\n",
    "    <td style=\"width: 50%; vertical-align: top;\">\n",
    "        <h2><strong>Flatten</strong></h2>\n",
    "        <p><strong>Rôle :</strong> Transforme la matrice 2D (sortie de la couche de pooling) en un vecteur 1D. Cela permet de passer d'une représentation spatiale à une représentation plate, adaptée pour les couches denses.</p>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td style=\"width: 50%\">\n",
    "        <img src=\"../../images/Layer_dense.png\" alt=\"Layer_dense\" style=\"width: 100%\"/>\n",
    "    </td>\n",
    "    <td style=\"width: 50%; vertical-align: top;\">\n",
    "        <h2><strong>Dense</strong></h2>\n",
    "        <p><strong>Neurones :</strong> 128</p>\n",
    "        <p><strong>Activation :</strong> 'relu'</p>\n",
    "        <p><strong>Rôle :</strong> Interprète les caractéristiques apprises par les couches précédentes pour effectuer une classification plus complexe. Les 128 neurones permettent au réseau de créer des combinaisons complexes et non linéaires des caractéristiques extraites.</p>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd0aac-be78-46b2-96e7-041d54c7da48",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre43\"></a>\n",
    "### Model python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b69e67da-6bd2-48d6-a908-49bde6250e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, routings=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", \n",
    "                                      shape=[self.num_capsules, input_shape[-1], self.dim_capsule],\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Expanding the dimensions to 4D to match with the kernel shape\n",
    "        inputs_expand = tf.expand_dims(inputs, 2)  # Change from [?, 1, 576, 64] to [?, 576, 1, 64]\n",
    "        inputs_expand = tf.expand_dims(inputs_expand, 1)  # Now shape is [?, 1, 576, 1, 64]\n",
    "    \n",
    "        # Tiling the inputs for the capsules\n",
    "        inputs_tiled = tf.tile(inputs_expand, [1, self.num_capsules, 1, 1, 1])  # Adjust the multiples accordingly\n",
    "    \n",
    "        # Reshaping the kernel for multiplication\n",
    "        kernel_reshaped = tf.reshape(self.kernel, [self.num_capsules, inputs.shape[-1], self.dim_capsule])\n",
    "    \n",
    "        # Capsule computation\n",
    "        inputs_hat = tf.map_fn(lambda x: tf.matmul(inputs_tiled, x), elems=kernel_reshaped)\n",
    "        # Implement dynamic routing here\n",
    "    \n",
    "        return tf.squeeze(inputs_hat, axis=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca1d536a-68b8-43f5-bd1a-643b675f8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_capsnet(input_shape, n_class):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\")(inputs)\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
    "    x = layers.Reshape((-1, 64))(x)\n",
    "    capsule = CapsuleLayer(num_capsules=n_class, dim_capsule=16)(x)\n",
    "    outputs = layers.Lambda(lambda z: tf.sqrt(tf.reduce_sum(tf.square(z), axis=2)))(capsule)\n",
    "    return models.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38c2c4-b18d-4ba3-9ed9-78270c914335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ae2a178-7eba-4039-877c-72124cd9e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node Equal defined at (most recent call last):\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1077, in launch_instance\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Temp\\ipykernel_34516\\190773957.py\", line 3, in <module>\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1155, in train_step\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1249, in compute_metrics\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 620, in update_state\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 723, in update_state\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\metrics\\accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 969, in sparse_categorical_matches\n\nIncompatible shapes: [64] vs. [10,64,576]\n\t [[{{node Equal}}]] [Op:__inference_train_function_6633]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m create_capsnet(input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m], n_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node Equal defined at (most recent call last):\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1077, in launch_instance\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n\n  File \"C:\\Users\\alexis.soltysiak\\AppData\\Local\\Temp\\ipykernel_34516\\190773957.py\", line 3, in <module>\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1155, in train_step\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1249, in compute_metrics\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 620, in update_state\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 723, in update_state\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\metrics\\accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n\n  File \"C:\\Users\\alexis.soltysiak\\Documents\\GitHub\\IA-NeuralNetwork\\venv\\lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 969, in sparse_categorical_matches\n\nIncompatible shapes: [64] vs. [10,64,576]\n\t [[{{node Equal}}]] [Op:__inference_train_function_6633]"
     ]
    }
   ],
   "source": [
    "model = create_capsnet(input_shape=[28, 28, 1], n_class=10)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b68e62-4a8f-4e19-ad9c-7f2301340f95",
   "metadata": {},
   "source": [
    "<a id=\"mon-titre5\"></a>\n",
    "## Evaluation of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb52e74-83d9-4b3b-80ac-693517b00d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3378 - accuracy: 0.9066\n",
      "Test accuracy: 0.9065999984741211\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92fa43-7a75-494b-ace7-f277afa5c384",
   "metadata": {},
   "source": [
    "<a id=\"mon-titre6\"></a>\n",
    "## Test on value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5caa4-1e30-4ffa-bf02-9db1748dd600",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre61\"></a>\n",
    "\n",
    "### Classe name list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ef73452-6428-4ff5-bfb6-67b30fd156e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451c634-d828-4085-bb62-ecf889daff50",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre62\"></a>\n",
    "### Select random image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "889e9668-cbbe-4d38-90a8-163394364368",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.randint(0, len(test_images))\n",
    "selected_image = test_images[random_index].reshape(1, 28, 28, 1)\n",
    "selected_label = test_labels[random_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7ef0c-e39e-4e19-9a7a-cababb8ccaa1",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre63\"></a>\n",
    "### Plot the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7f0c570-2314-4a45-9a9b-65c8da794eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(selected_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Image de Test Choisie')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71a8b3-222c-45b2-a861-26a2f2af73c4",
   "metadata": {},
   "source": [
    "<a id=\"mon-sous-titre64\"></a>\n",
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4ca3a6e-7d58-4179-80f3-deb3987d1a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n",
      "Classe prédite: 7\n",
      "Classe réelle: Sneaker\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(selected_image)\n",
    "predicted_class = np.argmax(prediction, axis=1)\n",
    "\n",
    "print(\"Classe prédite:\", predicted_class[0])\n",
    "print(\"Classe réelle:\", class_names[selected_label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
